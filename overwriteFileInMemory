## This script will automate reading a file, writing it out, reading it to memory, replacing the file in S3, then writing the file.
## It is meant for testing behavior.

import json
import os
import boto3
from pyspark.context import SparkContext
from awsglue.transforms import *
from awsglue.context import GlueContext
glueContext = GlueContext(sc)
sc = SparkContext.getOrCreate()

## Fill in these variables! 
myDb = "csv"
myTable = "csv_policestats"
s3temp = "s3://my-glue/temp"


print("1. Read as DynFrame from catalog")
dyf0 = glueContext.create_dynamic_frame.from_catalog(database = myDb, table_name = myTable)

print("2. Write to S3 as json")
glueContext.write_dynamic_frame.from_options(frame = dyf0, connection_type = "s3", connection_options = {"path": s3temp + "/json/"}, format = "json")

print("3. Import OS and run a move loop for S3 json files")
os.system('FILES0=`aws s3 ls s3://my-glue/temp/json/ | grep -o run.*`; for i in $FILES0; do aws s3 mv s3://my-glue/temp/json/$i s3://my-glue/temp/json/json_overwrite1.json; done;')

print("4. Read the new parquet file to DF")
dyf1 = glueContext.create_dynamic_frame.from_options('s3',{'paths': ['s3://my-glue/temp/json/json_overwrite1']},'json')

print("5. While that JSON file is in memory, re-run the CSV to JSON operation")
glueContext.write_dynamic_frame.from_options(frame = dyf0, connection_type = "s3", connection_options = {"path": s3temp + "/json/"}, format = "json")

print("6. Import OS and run a move loop for S3 json files AGAIN to overwrite")
os.system('FILES0=`aws s3 ls s3://my-glue/temp/json/ | grep -o run.*`; for i in $FILES0; do aws s3 mv s3://my-glue/temp/json/$i s3://my-glue/temp/json/json_overwrite1.json; done;')

print("7. Write to new Parquet file to new S3 location AGAIN")
glueContext.write_dynamic_frame.from_options(frame = dyf1, connection_type = "s3", connection_options = {"path":  s3temp + "/parquet"}, format = "parquet")

print("All done")
